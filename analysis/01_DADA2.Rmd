---
title: "Infer ASVs with DADA2"
date: "`r Sys.Date()`"
output: html_document
  toc: yes
  toc_float:
      collapsed: no
      smooth_scroll: yes
      toc_depth: 3
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      fig.align = "center", 
                      fig.path = "../figures/01_DADA2/") # send any figure output to this folder
```


# Before you start

## Set my seed
```{r set-seed}
#Any number can be chosen
set.seed(222)
```

#Goals of this file

1. Use raw fastq files and generate quality plots to assess quality of reads.
2. Filter and trim out bad sequences and bases from our sequencing files.
3. Write out fastq files with high quality sequences.
4. Evaluate the quality from our filter and trim 
5. Infer Errors on forward and reverse reads individually
6. Identified ASVs on forward and reverse reads separately, using the error model.
7. Merge forward and reverse ASVs into "contiguous ASVs"
8. Generate the ASV count table. (`otu_table` input for phyloseq.).


Output that we need
1. ASV Count Table: `otu_table`
2. Taxonomy Table: `tax_table`
3. Sample Information: `sample_data` track the reads lost throughout the DADA2 workflow

# Load Libraries
```{r load-libraries}
#install.packages("devtools")
library(devtools)

#devtools::install_github("benjjneb/dada2")
library(dada2)

#install.packages("tidyverse")
library(tidyverse)
```


# Load Data
```{r load-data}
# Set the raw fastq path to the raw sequencing files
# Path to the fastq files
raw_fastqs_path <- "data/01_DADA2/01_raw_gzipped_fastqs"
raw_fastqs_path

# What files are in this path? Intuition check
head(list.files(raw_fastqs_path))

# How many files are there?
str(list.files(raw_fastqs_path))

# Create a vector of forward reads
forward_reads <- list.files(raw_fastqs_path, pattern = "R1_001.fastq.gz", full.names = TRUE)
#Intuition check
head(forward_reads)

# Create a vector of reverse reads
reverse_reads <- list.files(raw_fastqs_path, pattern = "R2_001.fastq.gz", full.names = TRUE)
#Intuition check
head(reverse_reads)
```

# Quality Plots
```{r raw-quality-plot}
# Randomly selesct 2 samples from dataset to evaluate
random_samples <- sample(1:length(reverse_reads), size = 2)
random_samples

# Calculate and plot quality of these two samples
plotQualityProfile (forward_reads[random_samples])
 labs(title = "Forward Read Raw Quality")
 
plotQualityProfile (reverse_reads[random_samples])
  labs(title = "Reverse Read Raw Quality")
```

# Prepare a placeholder for filtered reads
```{r prep-filtred-sequrnces}
# vector of our samples, extract sample name from files
samples <- sapply(strsplit(basename(forward_reads), "_"), `[`,1)
# Inituition check
head(samples)

# Place filtered reads into the filtered_fastqs_path
filtered_fastqs_path <- "data/01_DADA2/02_filtered_fastqs"
filtered_fastqs_path


# create 2 variables: filtered_F, filtered_R
filtered_forward_reads <- file.path(filtered_fastqs_path, paste0(samples, "_R1_filtered.fastq.gz"))
length(filtered_forward_reads)

filtered_reverse_reads <- file.path(filtered_fastqs_path, paste0(samples, "_R2_filtered.fastq.gz"))
head(filtered_reverse_reads)
```

# Filter and trim reads

Parameters of filter and trim **DEPEND ON THE DATASET**

- 'maxN' = number of N bases. Remove all Ns from the data
- 'maxEE' = Quality filtering threshold applied to expected errors. Here, if there's 2 expected errors, it's ok. But more than 2. Throw away the sequence.
Two values, first is for forward reads; second is for reverse reads.
- 'trimLeft' = remove first three basepairs
- 'truncQ'

```{r filter-and-trim}
#Assign a vector to filtered reads
# trim out poor bases, first 3 bps on F reads
filtered_reads <-
filterAndTrim(fwd = forward_reads, filt = filtered_forward_reads,
              rev = reverse_reads, filt.rev = filtered_reverse_reads,
              maxN = 0, maxEE = c(2,2), trimLeft=3, 
              truncQ =2, rm.phix = TRUE, compress = TRUE) #multithread = TRUE)
```

## Trimmed Quality Plots
```{r filterTrim-quality-plots}
plotQualityProfile (filtered_forward_reads[random_samples])+
  labs(title = "Trimmed Forward Read Quality")

plotQualityProfile (filtered_reverse_reads[random_samples])+
  labs(title = "Trimmed Reverse Read Quality")
```

## Aggregared Trimmed Plots
```{r}
#plotQualityProfile(filtered_forward_reads, aggregate = TRUE)+
#  plotQualityProfile(filtered_reverse_reads, aggregate = TRUE)
```

## Stats on read output from 'filterAndTrim
```{r filterTrim-stats}
str(filtered_reads)
# Make output into dataframe
filtered_df <- as.data.frame(filtered_reads)
head(filtered_df)

# calculate some stats
filtered_df %>%
  reframe(median_reads_in = median(reads.in),
           median_reads_out = median(reads.out),
            median_percent_retained = (median(reads.out)/median(reads.in)))
```
        
          
# Error modeling
**NOTE:** Run separatey on each illumina dataset
```{r learn-errors}
# Forward reads
error_forward_reads <-
  learnErrors(filtered_forward_reads) #multithread = TRUE
# Plot Forward
plotErrors(error_forward_reads, nominalQ = TRUE) +
  labs(title = "Forward Read Error Model")

# Reverse reads
error_reverse_reads <-
  learnErrors(filtered_reverse_reads) #multithread = TRUE
# Plot Reverse
plotErrors(error_reverse_reads, nominalQ = TRUE) +
  labs(title = "Reverse Read Error Model")

```


# Infer ASVs

Note that this is happening separately on the forward and reverse reads! This is unique to DADA2
```{r infer_ASVs}
# Infer forward ASVs
dada_forward <- dada(filtered_forward_reads, 
                     err = error_forward_reads) #miltithread = TRUE

# Infer reverse ASVs
dada_reverse<- dada(filtered_reverse_reads, 
                     err = error_reverse_reads) #miltithread = TRUE
```


# Merge Forward and Reverse ASVs
```{r merge-ASVs}
# merge forward and reverse ASVs
merged_ASVs <- mergePairs(dada_forward, filtered_forward_reads,
                          dada_reverse, filtered_reverse_reads,
                          verbose = TRUE)
# Evaluate the output

typeof(merged_ASVs)
length(merged_ASVs)
names(merged_ASVs)
```

# Generate ASV Count Table
```{r generate-ASV-table}
# Create the ASV Count Table
raw_ASV_table <- makeSequenceTable(merged_ASVs)

# Write out the file to data/01_DADA2


```

# Session Information
```{r session-info}
#Ensure reproducibility
devtools::session_info()
```
